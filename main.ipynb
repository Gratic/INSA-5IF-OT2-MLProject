{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of the project is to recognize if the image is a face or not.\n",
    "\n",
    "Images are greyscale 36x36 pixels images.\n",
    "\n",
    "To reach the goal, we will try to train a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deep_learning_project.load_data import basic_load, imbalanced_load, get_transform, get_both_transform\n",
    "from deep_learning_project.net import FirstNeuralNetwork, LinearRegressionNetwork, SecondNeuralNetwork\n",
    "from deep_learning_project.torchsampler import ImbalancedDatasetSampler\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from deep_learning_project.trainers import BaseTrainer\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from deep_learning_project.utils import Exporter\n",
    "import cv2 as cv\n",
    "from cv2 import IMREAD_GRAYSCALE, IMREAD_COLOR\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "CURRENT_FOLDER = '.'\n",
    "MODEL_FOLDERS = os.path.join(CURRENT_FOLDER, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    obj = None\n",
    "    with torch.no_grad():\n",
    "        # exploit the model\n",
    "        logits = model(img)\n",
    "        pred_probab = nn.Softmax(dim=1)(logits)\n",
    "        y_pred = pred_probab.argmax(1).item() # indice(s) of the maximum value in the tensor\n",
    "        obj = (y_pred, pred_probab)\n",
    "    return obj\n",
    "\n",
    "def load_background_images():\n",
    "    TEXTURE_FOLDER = os.path.abspath('./deep_learning_project/textures/')\n",
    "\n",
    "    onlyfiles = [f for f in os.listdir(TEXTURE_FOLDER) if os.path.isfile(os.path.join(TEXTURE_FOLDER, f))]\n",
    "\n",
    "    img_datas = []\n",
    "\n",
    "    for filename in onlyfiles:\n",
    "        img_src = cv.samples.findFile(os.path.join(TEXTURE_FOLDER, filename))\n",
    "        img_datas.append(cv.imread(img_src, IMREAD_COLOR))\n",
    "    \n",
    "    return img_datas\n",
    "\n",
    "def gather_false_positive(model, background_images, rescale=0.8, threshold=0.8, stride=1, device='cpu'):\n",
    "    images = [] # this array will contain false positives images\n",
    "\n",
    "    for image in background_images:\n",
    "        transform = get_transform()\n",
    "        transformed_image = transform(T.ToPILImage()(image))\n",
    "\n",
    "        while (True):\n",
    "            for y in range(0, transformed_image.size()[1] - 36, stride):\n",
    "                for x in range(0, transformed_image.size()[2] - 36, stride):\n",
    "\n",
    "                    # crop and preparing the cropped image\n",
    "                    new_img = transformed_image[:, y:y+36, x:x+36]\n",
    "                    torch_new_img = new_img.reshape((1, 1, 36, 36))\n",
    "\n",
    "                    (y_pred, pred_probab) = predict(model, torch_new_img.to(device))\n",
    "                    \n",
    "                    # 0 = noface, 1 = face\n",
    "                    if pred_probab.squeeze()[1] >= threshold:\n",
    "                        images.append(new_img.reshape(36, 36))\n",
    "\n",
    "            new_height = math.ceil(transformed_image.size()[1] * rescale)\n",
    "            new_width = math.ceil(transformed_image.size()[2] * rescale)\n",
    "\n",
    "            # stop the loop if the image is smaller than the retina\n",
    "            if new_height < 36 or new_width < 36:\n",
    "                break\n",
    "\n",
    "            transformed_image = T.Resize((new_height, new_width), interpolation=InterpolationMode.BILINEAR)(transformed_image)\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    images = images.reshape(images.size()[0], 1, 36, 36).permute((0, 2, 3, 1)).numpy()\n",
    "    return images\n",
    "\n",
    "TEXTURE_FP_FOLDER = os.path.abspath('./deep_learning_project/texturesfp/')\n",
    "def save_images(dir_path, images):\n",
    "    dir_path = os.path.join(dir_path, '0')\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    offset = len(os.listdir(dir_path))\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        filename = str(offset + i) + \".pgm\"\n",
    "        cv.imwrite(os.path.join(dir_path, filename), img)\n",
    "\n",
    "def delete_files_in_dir(dir_path):\n",
    "    onlyfiles = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "    for file in onlyfiles:\n",
    "        os.remove(os.path.join(dir_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "learning_rate=0.001\n",
    "momentum=0.90\n",
    "weight_decay=0\n",
    "valid_size=0.2\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data is separated in 3 datasets.\n",
    "\n",
    "Train : to train the ML model.\n",
    "\n",
    "Valid : to valid the ML model.\n",
    "\n",
    "Test : to test the ML model.\n",
    "\n",
    "What is the difference between valid and test datasets. The main differencec is when there are used : valid are used inside the training process but test are used when the training is complete. Why use different datasets to do the same thing (test the generalization of model) ? Some do the validation with the test dataset but it is not scientifically correct because it will include a bias on the model. If we train the model until the test dataset error is the lowest, we effectively train the model for the test dataset... This is why we use two different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0. Parallel=False.\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "parallel = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        parallel = True\n",
    "\n",
    "print(f\"Running on {device}. Parallel={parallel}.\")\n",
    "\n",
    "data = imbalanced_load(valid_size=valid_size, batch_size=batch_size, device=device)\n",
    "train_loader = data[0]\n",
    "valid_loader = data[1]\n",
    "test_loader = data[2]\n",
    "classes = data[3]\n",
    "train_data = data[4]\n",
    "train_idx = data[5]\n",
    "\n",
    "background_images = load_background_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 36, 36])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(data[0]))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze() # from 2d to 1d, works only when the data is 1d [[x]] => [x]\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing one image of the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the FirstNeuralNetwork network\n",
    "\n",
    "We initialize the network.\n",
    "\n",
    "Print the configuration.\n",
    "\n",
    "Then predict a random input.\n",
    "\n",
    "---\n",
    "\n",
    "FirstNeuralNetwork is the neural network given by the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SecondNeuralNetwork(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 48, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=4800, out_features=48, bias=True)\n",
       "  (fc2): Linear(in_features=48, out_features=24, bias=True)\n",
       "  (fc3): Linear(in_features=24, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = SecondNeuralNetwork()\n",
    "if parallel:\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "random_shit = torch.rand((1, 1, 36, 36), device=device)\n",
    "\n",
    "logits = model(random_shit)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter = Exporter()\n",
    "exporter.prepare_export(MODEL_FOLDERS, str(model.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "trainer = BaseTrainer(model, loss_fn, optimizer, checkpoints_path=exporter.folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset=73376, train batches=1147, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [01:12, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 of 20 : train_loss: 0.29854, train_accuracy: 86.37%, valid_loss: 0.15196, valid_accuracy: 93.99%, test_loss: 0.22483, test_accuracy: 90.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [01:18, 14.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 of 20 : train_loss: 0.14301, train_accuracy: 94.43%, valid_loss: 0.10275, valid_accuracy: 96.16%, test_loss: 0.21639, test_accuracy: 92.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:43, 26.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 20 : train_loss: 0.12368, train_accuracy: 95.31%, valid_loss: 0.10574, valid_accuracy: 95.98%, test_loss: 0.56752, test_accuracy: 84.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:32, 35.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 of 20 : train_loss: 0.10685, train_accuracy: 95.91%, valid_loss: 0.09331, valid_accuracy: 96.48%, test_loss: 0.29331, test_accuracy: 93.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:27, 41.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 of 20 : train_loss: 0.09553, train_accuracy: 96.34%, valid_loss: 0.08826, valid_accuracy: 96.55%, test_loss: 0.29872, test_accuracy: 92.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:25, 45.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 of 20 : train_loss: 0.09089, train_accuracy: 96.57%, valid_loss: 0.07134, valid_accuracy: 97.44%, test_loss: 0.68914, test_accuracy: 80.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:48, 23.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 of 20 : train_loss: 0.08389, train_accuracy: 96.89%, valid_loss: 0.06334, valid_accuracy: 97.58%, test_loss: 0.55777, test_accuracy: 91.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:50, 22.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 of 20 : train_loss: 0.07927, train_accuracy: 97.00%, valid_loss: 0.07185, valid_accuracy: 97.30%, test_loss: 0.47567, test_accuracy: 88.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:34, 33.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 of 20 : train_loss: 0.07359, train_accuracy: 97.33%, valid_loss: 0.06243, valid_accuracy: 97.67%, test_loss: 0.52189, test_accuracy: 90.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:28, 40.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 of 20 : train_loss: 0.06989, train_accuracy: 97.40%, valid_loss: 0.07705, valid_accuracy: 97.06%, test_loss: 0.47341, test_accuracy: 87.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:25, 45.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 of 20 : train_loss: 0.06696, train_accuracy: 97.50%, valid_loss: 0.11250, valid_accuracy: 95.97%, test_loss: 0.74530, test_accuracy: 86.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:23, 48.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 of 20 : train_loss: 0.06596, train_accuracy: 97.53%, valid_loss: 0.07760, valid_accuracy: 97.09%, test_loss: 0.58170, test_accuracy: 88.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:23, 49.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 of 20 : train_loss: 0.06183, train_accuracy: 97.71%, valid_loss: 0.05730, valid_accuracy: 97.99%, test_loss: 2.64165, test_accuracy: 63.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 50.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 of 20 : train_loss: 0.06223, train_accuracy: 97.71%, valid_loss: 0.10741, valid_accuracy: 96.20%, test_loss: 1.04561, test_accuracy: 87.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 50.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 of 20 : train_loss: 0.05754, train_accuracy: 97.92%, valid_loss: 0.04980, valid_accuracy: 98.17%, test_loss: 0.79703, test_accuracy: 82.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:23, 49.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 of 20 : train_loss: 0.05585, train_accuracy: 97.94%, valid_loss: 0.08156, valid_accuracy: 97.08%, test_loss: 1.10146, test_accuracy: 81.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 50.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 of 20 : train_loss: 0.05254, train_accuracy: 98.05%, valid_loss: 0.04513, valid_accuracy: 98.35%, test_loss: 1.28065, test_accuracy: 76.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 51.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 of 20 : train_loss: 0.05172, train_accuracy: 98.15%, valid_loss: 0.06154, valid_accuracy: 97.75%, test_loss: 0.93840, test_accuracy: 86.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 50.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 of 20 : train_loss: 0.05152, train_accuracy: 98.08%, valid_loss: 0.04565, valid_accuracy: 98.47%, test_loss: 0.82280, test_accuracy: 86.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 51.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 of 20 : train_loss: 0.05141, train_accuracy: 98.17%, valid_loss: 0.07379, valid_accuracy: 97.18%, test_loss: 1.02656, test_accuracy: 86.81%\n",
      "CPU times: total: 3min 15s\n",
      "Wall time: 21min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            test_loader=test_loader,\n",
    "            epochs=epochs,\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_equal_or_less(dataset, num_samples):\n",
    "    # find indices or face in training set and shuffle the indices\n",
    "    indices = [idx for idx, target in enumerate(dataset.targets) if target == 1]\n",
    "    indices = [idx for idx in indices if idx in train_idx]\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # use the same number if fp_data is small or use the whole set\n",
    "    if num_samples < len(indices):\n",
    "        indices = indices[:num_samples]\n",
    "    \n",
    "    return torch.utils.data.Subset(dataset, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting bootstart threshold=0.9\n",
      "Size of train dataset=1051423, train batches=16429, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:37, 18.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 of 20 : train_loss: 0.01061, train_accuracy: 99.65%, valid_loss: 0.18335, valid_accuracy: 91.98%, test_loss: 1.56962, test_accuracy: 78.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:05, 19.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 of 20 : train_loss: 0.01019, train_accuracy: 99.67%, valid_loss: 0.09296, valid_accuracy: 96.53%, test_loss: 1.75537, test_accuracy: 86.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:04, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 20 : train_loss: 0.00960, train_accuracy: 99.69%, valid_loss: 0.14127, valid_accuracy: 95.06%, test_loss: 2.30260, test_accuracy: 72.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:33, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 of 20 : train_loss: 0.00955, train_accuracy: 99.69%, valid_loss: 0.11304, valid_accuracy: 95.87%, test_loss: 3.11687, test_accuracy: 67.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:24, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 of 20 : train_loss: 0.00961, train_accuracy: 99.70%, valid_loss: 0.06864, valid_accuracy: 97.73%, test_loss: 4.07620, test_accuracy: 71.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:32, 18.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 of 20 : train_loss: 0.00941, train_accuracy: 99.71%, valid_loss: 0.13253, valid_accuracy: 96.13%, test_loss: 7.10415, test_accuracy: 66.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:25, 18.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 of 20 : train_loss: 0.00948, train_accuracy: 99.72%, valid_loss: 0.17127, valid_accuracy: 95.32%, test_loss: 8.35604, test_accuracy: 60.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:29, 18.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 of 20 : train_loss: 0.00923, train_accuracy: 99.72%, valid_loss: 0.13226, valid_accuracy: 95.50%, test_loss: 7.97874, test_accuracy: 71.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:16, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 of 20 : train_loss: 0.00995, train_accuracy: 99.71%, valid_loss: 0.15416, valid_accuracy: 95.24%, test_loss: 5.33818, test_accuracy: 86.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:16, 19.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 of 20 : train_loss: 0.01213, train_accuracy: 99.65%, valid_loss: 0.10612, valid_accuracy: 96.44%, test_loss: 10.57249, test_accuracy: 84.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:04, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 of 20 : train_loss: 0.00907, train_accuracy: 99.73%, valid_loss: 0.08838, valid_accuracy: 97.14%, test_loss: 26.40368, test_accuracy: 91.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16429it [14:51, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 of 20 : train_loss: 0.01153, train_accuracy: 99.68%, valid_loss: 0.07614, valid_accuracy: 97.35%, test_loss: 5.15068, test_accuracy: 70.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9545it [08:36, 18.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alexis Strappazzon\\Documents\\git\\INSA-5IF-OT2-MLProject\\main.ipynb Cellule 17\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alexis%20Strappazzon/Documents/git/INSA-5IF-OT2-MLProject/main.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m4\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpin_memory\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m} \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m device \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alexis%20Strappazzon/Documents/git/INSA-5IF-OT2-MLProject/main.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(both_data, batch_size\u001b[39m=\u001b[39mbatch_size, sampler\u001b[39m=\u001b[39mboth_sampler, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Alexis%20Strappazzon/Documents/git/INSA-5IF-OT2-MLProject/main.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_loader\u001b[39m=\u001b[39;49mloader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alexis%20Strappazzon/Documents/git/INSA-5IF-OT2-MLProject/main.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         valid_loader\u001b[39m=\u001b[39;49mvalid_loader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alexis%20Strappazzon/Documents/git/INSA-5IF-OT2-MLProject/main.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         test_loader\u001b[39m=\u001b[39;49mtest_loader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alexis%20Strappazzon/Documents/git/INSA-5IF-OT2-MLProject/main.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alexis%20Strappazzon/Documents/git/INSA-5IF-OT2-MLProject/main.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alexis%20Strappazzon/Documents/git/INSA-5IF-OT2-MLProject/main.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alexis%20Strappazzon/Documents/git/INSA-5IF-OT2-MLProject/main.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m trainer\u001b[39m.\u001b[39m_print_evaluation(valid_loader, test_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alexis%20Strappazzon/Documents/git/INSA-5IF-OT2-MLProject/main.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Alexis Strappazzon\\Documents\\git\\INSA-5IF-OT2-MLProject\\deep_learning_project\\trainers.py:64\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[1;34m(self, train_loader, valid_loader, test_loader, epochs, device, verbose)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_epoch \u001b[39m=\u001b[39m checkpoint[\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     63\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> 64\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loop(train_loader, device)\n\u001b[0;32m     66\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     67\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_valid_loop(valid_loader, device)\n",
      "File \u001b[1;32mc:\\Users\\Alexis Strappazzon\\Documents\\git\\INSA-5IF-OT2-MLProject\\deep_learning_project\\trainers.py:128\u001b[0m, in \u001b[0;36mBaseTrainer._train_loop\u001b[1;34m(self, dataloader, device)\u001b[0m\n\u001b[0;32m    125\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_num_batch\n\u001b[0;32m    126\u001b[0m train_loss, accuracy \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m--> 128\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(dataloader), disable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtunning):\n\u001b[0;32m    129\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    130\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1315\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m   1314\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m-> 1315\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1316\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1317\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1151\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1164\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1165\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python310\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\python310\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delete_files_in_dir(TEXTURE_FP_FOLDER)\n",
    "\n",
    "for threshold in [0.9, 0.8, 0.6, 0.2, 0]:\n",
    "    print(\"Starting bootstart threshold={0}\".format(str(threshold)))\n",
    "    fp_images = gather_false_positive(\n",
    "        model,\n",
    "        background_images,\n",
    "        rescale=0.8,\n",
    "        stride=36,\n",
    "        threshold=threshold,\n",
    "        device=device)\n",
    "    \n",
    "    save_images(TEXTURE_FP_FOLDER, fp_images)\n",
    "\n",
    "    fp_data = torchvision.datasets.ImageFolder(TEXTURE_FP_FOLDER, transform=get_both_transform())\n",
    "    \n",
    "    # face_imgs = make_dataset_equal_or_less(train_data, len(fp_data))\n",
    "    face_imgs = torch.utils.data.Subset(train_data, train_idx)\n",
    "\n",
    "    both_data = torch.utils.data.ConcatDataset([face_imgs, fp_data])\n",
    "    both_sampler = ImbalancedDatasetSampler(both_data, callback_get_label=(lambda x, y: x[y][1]))\n",
    "\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if 'cuda' in device else {}\n",
    "    loader = torch.utils.data.DataLoader(both_data, batch_size=batch_size, sampler=both_sampler, **kwargs)\n",
    "    \n",
    "    trainer.fit(train_loader=loader,\n",
    "            valid_loader=valid_loader,\n",
    "            test_loader=test_loader,\n",
    "            epochs=epochs,\n",
    "            device=device,\n",
    "            verbose=False)\n",
    "\n",
    "    trainer._print_evaluation(valid_loader, test_loader)\n",
    "    print()\n",
    "\n",
    "# fullset = torchvision.datasets.ImageFolder(TEXTURE_FP_FOLDER, transform=get_transform())\n",
    "# both_data_full = torch.utils.data.ConcatDataset([torch.utils.data.Subset(train_data, train_idx), fp_data])\n",
    "# full_both_sampler = ImbalancedDatasetSampler(both_data_full, callback_get_label=(lambda x, y: x[y][1]))\n",
    "# kwargs = {'num_workers': 4, 'pin_memory': True} if 'cuda' in device else {}\n",
    "# full_loader = torch.utils.data.DataLoader(both_data_full, batch_size=batch_size, sampler=full_both_sampler, **kwargs)\n",
    "\n",
    "# trainer.fit(train_loader=full_loader,\n",
    "#             valid_loader=valid_loader,\n",
    "#             test_loader=test_loader,\n",
    "#             epochs=epochs,\n",
    "#             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file_data = {\n",
    "    \"device\": device,\n",
    "    \"network\": str(model.__class__.__name__),\n",
    "    \"epochs_number\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"train_len_data\": len(train_loader.dataset), \n",
    "    \"test_len_data\": len(test_loader.dataset),\n",
    "    \"trainer\": str(trainer.__class__.__name__),\n",
    "    \"optimizer\": str(optimizer.__class__.__name__),\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"loss_function\": str(loss_fn.__class__.__name__),\n",
    "    \"performances\" : trainer.get_stats(),\n",
    "}\n",
    "\n",
    "exporter.export_stat_file(stats_file_data)\n",
    "\n",
    "exporter.export_model(model, 'weights.pt')\n",
    "exporter.export_best_models(trainer.get_best_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# ld_model = torch.load('model.pt')\n",
    "\n",
    "# test_loop(test_loader, ld_model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c19fa61d258bb2b35aae2ada233c33e2817c1ce895aa48acba720c6bf7cbe3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
