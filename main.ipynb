{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of the project is to recognize if the image is a face or not.\n",
    "\n",
    "Images are greyscale 36x36 pixels images.\n",
    "\n",
    "To reach the goal, we will try to train a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deep_learning_project.load_data import basic_load, imbalanced_load, get_transform, get_both_transform\n",
    "from deep_learning_project.net import FirstNeuralNetwork, LinearRegressionNetwork, SecondNeuralNetwork\n",
    "from deep_learning_project.torchsampler import ImbalancedDatasetSampler\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from deep_learning_project.trainers import BaseTrainer\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from deep_learning_project.utils import Exporter\n",
    "import cv2 as cv\n",
    "from cv2 import IMREAD_GRAYSCALE, IMREAD_COLOR\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "CURRENT_FOLDER = '.'\n",
    "MODEL_FOLDERS = os.path.join(CURRENT_FOLDER, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    obj = None\n",
    "    with torch.no_grad():\n",
    "        # exploit the model\n",
    "        logits = model(img)\n",
    "        pred_probab = nn.Softmax(dim=1)(logits)\n",
    "        y_pred = pred_probab.argmax(1).item() # indice(s) of the maximum value in the tensor\n",
    "        obj = (y_pred, pred_probab)\n",
    "    return obj\n",
    "\n",
    "def load_background_images():\n",
    "    TEXTURE_FOLDER = os.path.abspath('./deep_learning_project/textures/')\n",
    "\n",
    "    onlyfiles = [f for f in os.listdir(TEXTURE_FOLDER) if os.path.isfile(os.path.join(TEXTURE_FOLDER, f))]\n",
    "\n",
    "    img_datas = []\n",
    "\n",
    "    for filename in onlyfiles:\n",
    "        img_src = cv.samples.findFile(os.path.join(TEXTURE_FOLDER, filename))\n",
    "        img_datas.append(cv.imread(img_src, IMREAD_COLOR))\n",
    "    \n",
    "    return img_datas\n",
    "\n",
    "def gather_false_positive(model, background_images, rescale=0.8, threshold=0.8, stride=1, device='cpu'):\n",
    "    images = [] # this array will contain false positives images\n",
    "\n",
    "    for image in background_images:\n",
    "        transform = get_transform()\n",
    "        transformed_image = transform(T.ToPILImage()(image))\n",
    "\n",
    "        while (True):\n",
    "            for y in range(0, transformed_image.size()[1] - 36, stride):\n",
    "                for x in range(0, transformed_image.size()[2] - 36, stride):\n",
    "\n",
    "                    # crop and preparing the cropped image\n",
    "                    new_img = transformed_image[:, y:y+36, x:x+36]\n",
    "                    torch_new_img = new_img.reshape((1, 1, 36, 36))\n",
    "\n",
    "                    (y_pred, pred_probab) = predict(model, torch_new_img.to(device))\n",
    "                    \n",
    "                    # 0 = noface, 1 = face\n",
    "                    if pred_probab.squeeze()[1] >= threshold:\n",
    "                        images.append(new_img.reshape(36, 36))\n",
    "\n",
    "            new_height = math.ceil(transformed_image.size()[1] * rescale)\n",
    "            new_width = math.ceil(transformed_image.size()[2] * rescale)\n",
    "\n",
    "            # stop the loop if the image is smaller than the retina\n",
    "            if new_height < 36 or new_width < 36:\n",
    "                break\n",
    "\n",
    "            transformed_image = T.Resize((new_height, new_width), interpolation=InterpolationMode.BILINEAR)(transformed_image)\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    images = images.reshape(images.size()[0], 1, 36, 36).permute((0, 2, 3, 1)).numpy()\n",
    "    return images\n",
    "\n",
    "TEXTURE_FP_FOLDER = os.path.abspath('./deep_learning_project/texturesfp/')\n",
    "def save_images(dir_path, images):\n",
    "    dir_path = os.path.join(dir_path, '0')\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    offset = len(os.listdir(dir_path))\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        filename = str(offset + i) + \".pgm\"\n",
    "        cv.imwrite(os.path.join(dir_path, filename), img)\n",
    "\n",
    "def delete_files_in_dir(dir_path):\n",
    "    onlyfiles = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "    for file in onlyfiles:\n",
    "        os.remove(os.path.join(dir_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epochs=5\n",
    "epochs=20\n",
    "learning_rate=0.001\n",
    "momentum=0.90\n",
    "weight_decay=0\n",
    "valid_size=0.2\n",
    "batch_size=64\n",
    "\n",
    "bootstart_min_epoch=3\n",
    "bootstart_max_epoch=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data is separated in 3 datasets.\n",
    "\n",
    "Train : to train the ML model.\n",
    "\n",
    "Valid : to valid the ML model.\n",
    "\n",
    "Test : to test the ML model.\n",
    "\n",
    "What is the difference between valid and test datasets. The main differencec is when there are used : valid are used inside the training process but test are used when the training is complete. Why use different datasets to do the same thing (test the generalization of model) ? Some do the validation with the test dataset but it is not scientifically correct because it will include a bias on the model. If we train the model until the test dataset error is the lowest, we effectively train the model for the test dataset... This is why we use two different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0. Parallel=False.\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "parallel = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        parallel = True\n",
    "\n",
    "print(f\"Running on {device}. Parallel={parallel}.\")\n",
    "\n",
    "data = imbalanced_load(valid_size=valid_size, batch_size=batch_size, device=device)\n",
    "train_loader = data[0]\n",
    "valid_loader = data[1]\n",
    "test_loader = data[2]\n",
    "classes = data[3]\n",
    "train_data = data[4]\n",
    "train_idx = data[5]\n",
    "\n",
    "background_images = load_background_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 36, 36])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(data[0]))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze() # from 2d to 1d, works only when the data is 1d [[x]] => [x]\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing one image of the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the FirstNeuralNetwork network\n",
    "\n",
    "We initialize the network.\n",
    "\n",
    "Print the configuration.\n",
    "\n",
    "Then predict a random input.\n",
    "\n",
    "---\n",
    "\n",
    "FirstNeuralNetwork is the neural network given by the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SecondNeuralNetwork(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 48, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=4800, out_features=48, bias=True)\n",
       "  (fc2): Linear(in_features=48, out_features=24, bias=True)\n",
       "  (fc3): Linear(in_features=24, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = SecondNeuralNetwork()\n",
    "if parallel:\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "random_shit = torch.rand((1, 1, 36, 36), device=device)\n",
    "\n",
    "logits = model(random_shit)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter = Exporter()\n",
    "exporter.prepare_export(MODEL_FOLDERS, str(model.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "trainer = BaseTrainer(model, loss_fn, optimizer, checkpoints_path=exporter.folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            test_loader=test_loader,\n",
    "            min_epochs=min_epochs,\n",
    "            max_epochs=epochs,\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_equal_or_less(dataset, num_samples):\n",
    "    # find indices or face in training set and shuffle the indices\n",
    "    indices = [idx for idx, target in enumerate(dataset.targets) if target == 1]\n",
    "    indices = [idx for idx in indices if idx in train_idx]\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # use the same number if fp_data is small or use the whole set\n",
    "    if num_samples < len(indices):\n",
    "        indices = indices[:num_samples]\n",
    "    \n",
    "    return torch.utils.data.Subset(dataset, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_files_in_dir(TEXTURE_FP_FOLDER)\n",
    "\n",
    "for threshold in [0.9, 0.8, 0.6, 0.2, 0]:\n",
    "    print(\"Starting bootstart threshold={0}\".format(str(threshold)))\n",
    "    fp_images = gather_false_positive(\n",
    "        model,\n",
    "        background_images,\n",
    "        rescale=0.8,\n",
    "        stride=36,\n",
    "        threshold=threshold,\n",
    "        device=device)\n",
    "    \n",
    "    save_images(TEXTURE_FP_FOLDER, fp_images)\n",
    "\n",
    "    fp_data = torchvision.datasets.ImageFolder(TEXTURE_FP_FOLDER, transform=get_both_transform())\n",
    "    \n",
    "    # face_imgs = make_dataset_equal_or_less(train_data, len(fp_data))\n",
    "    face_imgs = torch.utils.data.Subset(train_data, train_idx)\n",
    "\n",
    "    both_data = torch.utils.data.ConcatDataset([face_imgs, fp_data])\n",
    "    both_sampler = ImbalancedDatasetSampler(both_data, callback_get_label=(lambda x, y: x[y][1]))\n",
    "\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if 'cuda' in device else {}\n",
    "    loader = torch.utils.data.DataLoader(both_data, batch_size=batch_size, sampler=both_sampler, **kwargs)\n",
    "    \n",
    "    trainer.fit(train_loader=loader,\n",
    "            valid_loader=valid_loader,\n",
    "            test_loader=test_loader,\n",
    "            min_epochs=bootstart_min_epoch,\n",
    "            max_epochs=bootstart_max_epoch,\n",
    "            device=device,\n",
    "            verbose=False)\n",
    "\n",
    "    trainer._print_evaluation(valid_loader, test_loader)\n",
    "    print()\n",
    "\n",
    "# fullset = torchvision.datasets.ImageFolder(TEXTURE_FP_FOLDER, transform=get_transform())\n",
    "# both_data_full = torch.utils.data.ConcatDataset([torch.utils.data.Subset(train_data, train_idx), fp_data])\n",
    "# full_both_sampler = ImbalancedDatasetSampler(both_data_full, callback_get_label=(lambda x, y: x[y][1]))\n",
    "# kwargs = {'num_workers': 4, 'pin_memory': True} if 'cuda' in device else {}\n",
    "# full_loader = torch.utils.data.DataLoader(both_data_full, batch_size=batch_size, sampler=full_both_sampler, **kwargs)\n",
    "\n",
    "# trainer.fit(train_loader=full_loader,\n",
    "#             valid_loader=valid_loader,\n",
    "#             test_loader=test_loader,\n",
    "#             epochs=epochs,\n",
    "#             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file_data = {\n",
    "    \"device\": device,\n",
    "    \"network\": str(model.__class__.__name__),\n",
    "    \"epochs_number\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"train_len_data\": len(train_loader.dataset), \n",
    "    \"test_len_data\": len(test_loader.dataset),\n",
    "    \"trainer\": str(trainer.__class__.__name__),\n",
    "    \"optimizer\": str(optimizer.__class__.__name__),\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"loss_function\": str(loss_fn.__class__.__name__),\n",
    "    \"performances\" : trainer.get_stats(),\n",
    "}\n",
    "\n",
    "exporter.export_stat_file(stats_file_data)\n",
    "\n",
    "exporter.export_model(model, 'weights.pt')\n",
    "exporter.export_best_models(trainer.get_best_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# ld_model = torch.load('model.pt')\n",
    "\n",
    "# test_loop(test_loader, ld_model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c19fa61d258bb2b35aae2ada233c33e2817c1ce895aa48acba720c6bf7cbe3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
