{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of the project is to recognize if the image is a face or not.\n",
    "\n",
    "Images are greyscale 36x36 pixels images.\n",
    "\n",
    "To reach the goal, we will try to train a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deep_learning_project.load_data import basic_load, imbalanced_load, get_transform, get_both_transform\n",
    "from deep_learning_project.net import FirstNeuralNetwork, LinearRegressionNetwork, SecondNeuralNetwork, ThirdNeuralNetwork, FourthNeuralNetwork, FifthNeuralNetwork\n",
    "from deep_learning_project.torchsampler import ImbalancedDatasetSampler\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from deep_learning_project.trainers import BaseTrainer\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from deep_learning_project.utils import Exporter\n",
    "import cv2 as cv\n",
    "from cv2 import IMREAD_GRAYSCALE, IMREAD_COLOR\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from shutil import rmtree\n",
    "import time\n",
    "\n",
    "CURRENT_FOLDER = '.'\n",
    "MODEL_FOLDERS = os.path.join(CURRENT_FOLDER, 'models')\n",
    "TEXTURE_FP_FOLDER = os.path.abspath('./deep_learning_project/texturesfp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    obj = None\n",
    "    with torch.no_grad():\n",
    "        # exploit the model\n",
    "        logits = model(img)\n",
    "        pred_probab = nn.Softmax(dim=1)(logits)\n",
    "        \n",
    "        del logits\n",
    "        torch.cuda.empty_cache()\n",
    "        # y_pred = pred_probab.argmax(1).item() # indice(s) of the maximum value in the tensor\n",
    "        obj = pred_probab\n",
    "    return obj\n",
    "\n",
    "def load_background_images():\n",
    "    TEXTURE_FOLDER = os.path.abspath('./deep_learning_project/textures/')\n",
    "\n",
    "    onlyfiles = [f for f in os.listdir(TEXTURE_FOLDER) if os.path.isfile(os.path.join(TEXTURE_FOLDER, f))]\n",
    "\n",
    "    img_datas = []\n",
    "\n",
    "    for filename in onlyfiles:\n",
    "        img_src = cv.samples.findFile(os.path.join(TEXTURE_FOLDER, filename))\n",
    "        img_datas.append(cv.imread(img_src, IMREAD_COLOR))\n",
    "    \n",
    "    return img_datas\n",
    "\n",
    "def gather_false_positive(model, background_images, rescale=0.8, threshold=0.8, stride=1, limit=1000, per_image_limit=50, timelimit=1800, timelimit_per_image=600, shuffle=True, device='cpu'):\n",
    "    torch.cuda.empty_cache()\n",
    "    images = [] # this array will contain false positives images\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(background_images)\n",
    "\n",
    "    num_fp = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        i = 0\n",
    "        image = background_images[i]\n",
    "        transform = get_transform()\n",
    "        transformed_image = transform(T.ToPILImage()(image)).to(device)\n",
    "\n",
    "        nextImage = False\n",
    "        shouldStop = False\n",
    "        num_im = 0\n",
    "\n",
    "        while (True):\n",
    "            current_windows = []\n",
    "            time_image = time.time()\n",
    "            current_time = time_image\n",
    "            for y in range(0, transformed_image.size()[1] - 36, stride):\n",
    "                for x in range(0, transformed_image.size()[2] - 36, stride):\n",
    "\n",
    "                    # crop and preparing the cropped image\n",
    "                    new_img = transformed_image[:, y:y+36, x:x+36]\n",
    "                    # torch_new_img = new_img.reshape((1, 1, 36, 36))\n",
    "\n",
    "                    current_windows.append(new_img)\n",
    "\n",
    "                    current_time = time.time()\n",
    "                    if time_image + timelimit_per_image < current_time or start_time + timelimit < start_time + current_time - time_image:\n",
    "                        break\n",
    "\n",
    "                if time_image + timelimit_per_image < current_time  or start_time + timelimit < start_time + current_time - time_image:\n",
    "                        break\n",
    "                \n",
    "            for i in range(0, len(current_windows), 512):\n",
    "                current_torch = torch.stack(current_windows[i:i+512])\n",
    "\n",
    "                pred_probab = predict(model, current_torch.to(device))\n",
    "                \n",
    "                pred_cpu = pred_probab.cpu()\n",
    "\n",
    "                del current_torch\n",
    "                del pred_probab\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # 0 = noface, 1 = face\n",
    "                for j, pred in enumerate(pred_cpu):\n",
    "                    if pred[1] >= threshold:\n",
    "                        images.append(current_windows[j].cpu())\n",
    "                        num_fp += 1\n",
    "                        num_im += 1\n",
    "\n",
    "                        if num_fp >= limit:\n",
    "                            shouldStop = True\n",
    "                            break\n",
    "                        if num_im >= per_image_limit:\n",
    "                            nextImage = True\n",
    "                            break\n",
    "                \n",
    "                del pred_cpu\n",
    "                \n",
    "                if shouldStop or nextImage:\n",
    "                    break\n",
    "            \n",
    "            if shouldStop or nextImage:\n",
    "                    break\n",
    "\n",
    "            if start_time - time.time() > timelimit:\n",
    "                shouldStop = True\n",
    "                break\n",
    "\n",
    "            # scale down\n",
    "            new_height = math.ceil(transformed_image.size()[1] * rescale)\n",
    "            new_width = math.ceil(transformed_image.size()[2] * rescale)\n",
    "\n",
    "            # stop the loop if the image is smaller than the retina\n",
    "            if new_height < 36 or new_width < 36:\n",
    "                break\n",
    "\n",
    "            transformed_image = T.Resize((new_height, new_width), interpolation=InterpolationMode.BILINEAR)(transformed_image)\n",
    "        \n",
    "        if shouldStop:\n",
    "            break\n",
    "        \n",
    "        i += 1\n",
    "        i = (i % len(background_images))\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    images = images.reshape(images.size()[0], 1, 36, 36).permute((0, 2, 3, 1)).numpy()\n",
    "    images = (images*0.5 + 0.5) * 255\n",
    "    return images\n",
    "\n",
    "def save_images(dir_path, images):\n",
    "    dir_path = os.path.join(dir_path, '0')\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    offset = len(os.listdir(dir_path))\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        filename = str(offset + i) + \".pgm\"\n",
    "        cv.imwrite(os.path.join(dir_path, filename), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epochs=10\n",
    "epochs=20\n",
    "learning_rate=0.001\n",
    "momentum=0.90\n",
    "weight_decay=0\n",
    "valid_size=0.2\n",
    "batch_size=64\n",
    "\n",
    "bootstrap_min_epoch=1\n",
    "bootstrap_max_epoch=10\n",
    "bootstrap_lr = learning_rate * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data is separated in 3 datasets.\n",
    "\n",
    "Train : to train the ML model.\n",
    "\n",
    "Valid : to valid the ML model.\n",
    "\n",
    "Test : to test the ML model.\n",
    "\n",
    "What is the difference between valid and test datasets. The main differencec is when there are used : valid are used inside the training process but test are used when the training is complete. Why use different datasets to do the same thing (test the generalization of model) ? Some do the validation with the test dataset but it is not scientifically correct because it will include a bias on the model. If we train the model until the test dataset error is the lowest, we effectively train the model for the test dataset... This is why we use two different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0. Parallel=False.\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "parallel = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        parallel = True\n",
    "\n",
    "print(f\"Running on {device}. Parallel={parallel}.\")\n",
    "\n",
    "data = imbalanced_load(valid_size=valid_size, batch_size=batch_size, device=device)\n",
    "train_loader = data[0]\n",
    "valid_loader = data[1]\n",
    "test_loader = data[2]\n",
    "classes = data[3]\n",
    "train_data = data[4]\n",
    "train_idx = data[5]\n",
    "\n",
    "background_images = load_background_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing one image of the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the FirstNeuralNetwork network\n",
    "\n",
    "We initialize the network.\n",
    "\n",
    "Print the configuration.\n",
    "\n",
    "Then predict a random input.\n",
    "\n",
    "---\n",
    "\n",
    "FirstNeuralNetwork is the neural network given by the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FifthNeuralNetwork(\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Conv2d(256, 256, kernel_size=(6, 6), stride=(1, 1))\n",
       "  (fc2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (fc3): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = FifthNeuralNetwork()\n",
    "if parallel:\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter = Exporter()\n",
    "exporter.prepare_export(MODEL_FOLDERS, str(model.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "trainer = BaseTrainer(model, loss_fn, optimizer, checkpoints_path=exporter.folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset=73376, train batches=1147, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [01:30, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 of 20 : train_loss: 0.16940, train_accuracy: 93.06%, valid_loss: 0.05499, valid_accuracy: 98.11%, test_loss: 0.07714, test_accuracy: 97.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:41, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 of 20 : train_loss: 0.05045, train_accuracy: 98.29%, valid_loss: 0.03096, valid_accuracy: 98.99%, test_loss: 0.06133, test_accuracy: 98.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:28, 39.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 20 : train_loss: 0.03801, train_accuracy: 98.75%, valid_loss: 0.02634, valid_accuracy: 99.10%, test_loss: 0.05073, test_accuracy: 98.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:23, 48.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 of 20 : train_loss: 0.03022, train_accuracy: 98.98%, valid_loss: 0.01847, valid_accuracy: 99.42%, test_loss: 0.06569, test_accuracy: 98.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 51.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 of 20 : train_loss: 0.02716, train_accuracy: 99.16%, valid_loss: 0.01583, valid_accuracy: 99.53%, test_loss: 0.05048, test_accuracy: 98.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 51.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 of 20 : train_loss: 0.02216, train_accuracy: 99.31%, valid_loss: 0.02243, valid_accuracy: 99.30%, test_loss: 0.05164, test_accuracy: 98.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 51.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 of 20 : train_loss: 0.02011, train_accuracy: 99.38%, valid_loss: 0.01360, valid_accuracy: 99.48%, test_loss: 0.08283, test_accuracy: 97.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 51.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 of 20 : train_loss: 0.02172, train_accuracy: 99.38%, valid_loss: 0.01500, valid_accuracy: 99.54%, test_loss: 0.05684, test_accuracy: 98.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 52.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 of 20 : train_loss: 0.01863, train_accuracy: 99.44%, valid_loss: 0.01465, valid_accuracy: 99.51%, test_loss: 0.05544, test_accuracy: 98.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 51.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 of 20 : train_loss: 0.01865, train_accuracy: 99.42%, valid_loss: 0.01463, valid_accuracy: 99.57%, test_loss: 0.06665, test_accuracy: 98.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:22, 52.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 of 20 : train_loss: 0.02007, train_accuracy: 99.41%, valid_loss: 0.04679, valid_accuracy: 98.51%, test_loss: 0.11521, test_accuracy: 97.29%\n",
      "CPU times: total: 4min 37s\n",
      "Wall time: 11min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            test_loader=test_loader,\n",
    "            min_epochs=min_epochs,\n",
    "            max_epochs=epochs,\n",
    "            early_stopping='valid',\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_equal_or_less(dataset, num_samples):\n",
    "    # find indices or face in training set and shuffle the indices\n",
    "    indices = [idx for idx, target in enumerate(dataset.targets) if target == 1]\n",
    "    indices = [idx for idx in indices if idx in train_idx]\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # use the same number if fp_data is small or use the whole set\n",
    "    if num_samples < len(indices):\n",
    "        indices = indices[:num_samples]\n",
    "    \n",
    "    return torch.utils.data.Subset(dataset, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(TensorDataset):\n",
    "    def __init__(self, *tensors: torch.Tensor) -> None:\n",
    "        super().__init__(*tensors)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.tensors[0][index], self.tensors[1][index].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap_data = torch.utils.data.Subset(train_data, range(0,10))\n",
    "\n",
    "# fp_images = gather_false_positive(\n",
    "#     model,\n",
    "#     background_images,\n",
    "#     rescale=0.8,\n",
    "#     stride=36,\n",
    "#     threshold=0,\n",
    "#     limit=10,\n",
    "#     per_image_limit=5000/len(background_images),\n",
    "#     shuffle=True,\n",
    "#     device=device)\n",
    "    \n",
    "# fp_tensor = torch.Tensor(fp_images).permute((0,3,1,2))\n",
    "# nonface = torch.zeros(fp_tensor.size(0), dtype=int)\n",
    "\n",
    "# fp_data = CustomTensorDataset(fp_tensor, nonface)\n",
    "\n",
    "\n",
    "\n",
    "# both_data = torch.utils.data.ConcatDataset([bootstrap_data, fp_data])\n",
    "# both_sampler = ImbalancedDatasetSampler(both_data, callback_get_label=(lambda x, y: x[y][1] if type(x[y][1]) == int else x[y][1].item()))\n",
    "\n",
    "# kwargs = {'num_workers': 0, 'pin_memory': True} if 'cuda' in device else {}\n",
    "# loader = torch.utils.data.DataLoader(both_data, batch_size=batch_size, sampler=both_sampler, **kwargs)\n",
    "# loader = torch.utils.data.DataLoader(both_data, batch_size=batch_size, sampler=both_sampler)\n",
    "\n",
    "# trainer.optimizer = torch.optim.Adam(model.parameters(), lr=bootstrap_lr, weight_decay=weight_decay)\n",
    "# trainer.optimizer = torch.optim.SGD(model.parameters(), lr=bootstrap_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "# trainer.fit(train_loader=loader,\n",
    "#     valid_loader=valid_loader,\n",
    "#     test_loader=test_loader,\n",
    "#     min_epochs=bootstrap_min_epoch,\n",
    "#     max_epochs=bootstrap_max_epoch,\n",
    "#     early_stopping=None,\n",
    "#     device=device,\n",
    "#     verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting bootstrapping threshold=0.9\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mL'interruption du noyau a échoué. \n",
      "\u001b[1;31mImpossible de démarrer le noyau 'Python 3.10.8 64-bit' en raison du délai d’expiration de la connexion. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "if os.path.exists(TEXTURE_FP_FOLDER):\n",
    "    rmtree(TEXTURE_FP_FOLDER) # delete old bootstrap\n",
    "\n",
    "bootstrap_data = torch.utils.data.Subset(train_data, train_idx)\n",
    "\n",
    "for i, threshold in enumerate([0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0]):\n",
    "    print(\"Starting bootstrapping threshold={0}\".format(str(threshold)))\n",
    "    fp_images = gather_false_positive(\n",
    "        model,\n",
    "        background_images,\n",
    "        rescale=0.8,\n",
    "        stride=1,\n",
    "        threshold=threshold,\n",
    "        limit=5000,\n",
    "        per_image_limit=5000/len(background_images),\n",
    "        shuffle=True,\n",
    "        device=device)\n",
    "    \n",
    "    save_path = os.path.join(TEXTURE_FP_FOLDER, str(i))\n",
    "    \n",
    "    save_images(save_path, fp_images)\n",
    "\n",
    "    fp_tensor = torch.Tensor(fp_images).permute((0,3,1,2))\n",
    "    nonface = torch.zeros(fp_tensor.size(0), dtype=int)\n",
    "\n",
    "    fp_data = CustomTensorDataset(fp_tensor, nonface)\n",
    "\n",
    "    # fp_data = torchvision.datasets.ImageFolder(save_path, transform=get_both_transform())\n",
    "    \n",
    "    # face_imgs = make_dataset_equal_or_less(train_data, len(fp_data))\n",
    "    \n",
    "\n",
    "    both_data = torch.utils.data.ConcatDataset([bootstrap_data, fp_data])\n",
    "    both_sampler = ImbalancedDatasetSampler(both_data, callback_get_label=(lambda x, y: x[y][1]))\n",
    "\n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if 'cuda' in device else {}\n",
    "    loader = torch.utils.data.DataLoader(both_data, batch_size=batch_size, sampler=both_sampler, **kwargs)\n",
    "    \n",
    "    trainer.optimizer = torch.optim.Adam(model.parameters(), lr=bootstrap_lr, weight_decay=weight_decay)\n",
    "    # trainer.optimizer = torch.optim.SGD(model.parameters(), lr=bootstrap_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "    trainer.fit(train_loader=loader,\n",
    "        valid_loader=valid_loader,\n",
    "        test_loader=test_loader,\n",
    "        min_epochs=bootstrap_min_epoch,\n",
    "        max_epochs=bootstrap_max_epoch,\n",
    "        early_stopping=None,\n",
    "        device=device,\n",
    "        verbose=False)\n",
    "\n",
    "    trainer._print_evaluation(valid_loader, test_loader)\n",
    "    bootstrap_data = both_data\n",
    "    print()\n",
    "\n",
    "# fullset = torchvision.datasets.ImageFolder(TEXTURE_FP_FOLDER, transform=get_transform())\n",
    "# both_data_full = torch.utils.data.ConcatDataset([torch.utils.data.Subset(train_data, train_idx), fp_data])\n",
    "# full_both_sampler = ImbalancedDatasetSampler(both_data_full, callback_get_label=(lambda x, y: x[y][1]))\n",
    "# kwargs = {'num_workers': 4, 'pin_memory': True} if 'cuda' in device else {}\n",
    "# full_loader = torch.utils.data.DataLoader(both_data_full, batch_size=batch_size, sampler=full_both_sampler, **kwargs)\n",
    "\n",
    "# trainer.fit(train_loader=full_loader,\n",
    "#             valid_loader=valid_loader,\n",
    "#             test_loader=test_loader,\n",
    "#             epochs=epochs,\n",
    "#             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file_data = {\n",
    "    \"device\": device,\n",
    "    \"network\": str(model.__class__.__name__),\n",
    "    \"epochs_number\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"train_len_data\": len(train_loader.dataset), \n",
    "    \"test_len_data\": len(test_loader.dataset),\n",
    "    \"trainer\": str(trainer.__class__.__name__),\n",
    "    \"optimizer\": str(optimizer.__class__.__name__),\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"loss_function\": str(loss_fn.__class__.__name__),\n",
    "    \"performances\" : trainer.get_stats(),\n",
    "}\n",
    "\n",
    "exporter.export_stat_file(stats_file_data)\n",
    "\n",
    "exporter.export_model(model, 'weights.pt')\n",
    "exporter.export_best_models(trainer.get_best_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('models/20221127_2300_ThirdNeuralNetwork/best_valid_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp_data = torchvision.datasets.ImageFolder(TEXTURE_FP_FOLDER, transform=get_both_transform())\n",
    "    \n",
    "# # face_imgs = make_dataset_equal_or_less(train_data, len(fp_data))\n",
    "# face_imgs = torch.utils.data.Subset(train_data, train_idx)\n",
    "\n",
    "# both_data = torch.utils.data.ConcatDataset([face_imgs, fp_data])\n",
    "# both_sampler = ImbalancedDatasetSampler(both_data, callback_get_label=(lambda x, y: x[y][1]))\n",
    "\n",
    "# kwargs = {'num_workers': 4, 'pin_memory': True} if 'cuda' in device else {}\n",
    "# loader = torch.utils.data.DataLoader(both_data, batch_size=batch_size, sampler=both_sampler, **kwargs)\n",
    "\n",
    "# trainer.optimizer = torch.optim.Adam(model.parameters(), lr=0.000005, weight_decay=weight_decay)\n",
    "# # trainer.optimizer = torch.optim.SGD(model.parameters(), lr=bootstrap_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "# trainer.fit(train_loader=loader,\n",
    "#         valid_loader=valid_loader,\n",
    "#         test_loader=test_loader,\n",
    "#         min_epochs=bootstrap_min_epoch,\n",
    "#         max_epochs=bootstrap_max_epoch,\n",
    "#         early_stopping=None,\n",
    "#         device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# ld_model = torch.load('model.pt')\n",
    "\n",
    "# test_loop(test_loader, ld_model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c19fa61d258bb2b35aae2ada233c33e2817c1ce895aa48acba720c6bf7cbe3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
