{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of the project is to recognize if the image is a face or not.\n",
    "\n",
    "Images are greyscale 36x36 pixels images.\n",
    "\n",
    "To reach the goal, we will try to train a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deep_learning_project.load_data import basic_load, imbalanced_load, get_transform, get_both_transform\n",
    "from deep_learning_project.net import FirstNeuralNetwork, LinearRegressionNetwork, SecondNeuralNetwork, ThirdNeuralNetwork, FourthNeuralNetwork\n",
    "from deep_learning_project.torchsampler import ImbalancedDatasetSampler\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from deep_learning_project.trainers import BaseTrainer\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from deep_learning_project.utils import Exporter\n",
    "import cv2 as cv\n",
    "from cv2 import IMREAD_GRAYSCALE, IMREAD_COLOR\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "CURRENT_FOLDER = '.'\n",
    "MODEL_FOLDERS = os.path.join(CURRENT_FOLDER, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    obj = None\n",
    "    with torch.no_grad():\n",
    "        # exploit the model\n",
    "        logits = model(img)\n",
    "        pred_probab = nn.Softmax(dim=1)(logits)\n",
    "        y_pred = pred_probab.argmax(1).item() # indice(s) of the maximum value in the tensor\n",
    "        obj = (y_pred, pred_probab)\n",
    "    return obj\n",
    "\n",
    "def load_background_images():\n",
    "    TEXTURE_FOLDER = os.path.abspath('./deep_learning_project/textures/')\n",
    "\n",
    "    onlyfiles = [f for f in os.listdir(TEXTURE_FOLDER) if os.path.isfile(os.path.join(TEXTURE_FOLDER, f))]\n",
    "\n",
    "    img_datas = []\n",
    "\n",
    "    for filename in onlyfiles:\n",
    "        img_src = cv.samples.findFile(os.path.join(TEXTURE_FOLDER, filename))\n",
    "        img_datas.append(cv.imread(img_src, IMREAD_COLOR))\n",
    "    \n",
    "    return img_datas\n",
    "\n",
    "def gather_false_positive(model, background_images, rescale=0.8, threshold=0.8, stride=1, limit=1000, per_image_limit=50, shuffle=True, device='cpu'):\n",
    "    images = [] # this array will contain false positives images\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(background_images)\n",
    "\n",
    "    num_fp = 0\n",
    "    while True:\n",
    "        i = 0\n",
    "        image = background_images[i]\n",
    "        transform = get_transform()\n",
    "        transformed_image = transform(T.ToPILImage()(image))\n",
    "\n",
    "        nextImage = False\n",
    "        shouldStop = False\n",
    "        num_im = 0\n",
    "\n",
    "        while (True):\n",
    "            for y in range(0, transformed_image.size()[1] - 36, stride):\n",
    "                for x in range(0, transformed_image.size()[2] - 36, stride):\n",
    "\n",
    "                    # crop and preparing the cropped image\n",
    "                    new_img = transformed_image[:, y:y+36, x:x+36]\n",
    "                    torch_new_img = new_img.reshape((1, 1, 36, 36))\n",
    "\n",
    "                    (y_pred, pred_probab) = predict(model, torch_new_img.to(device))\n",
    "                    \n",
    "                    # 0 = noface, 1 = face\n",
    "                    if pred_probab.squeeze()[1] >= threshold:\n",
    "                        images.append(new_img.reshape(36, 36))\n",
    "                        num_fp += 1\n",
    "                        num_im += 1\n",
    "\n",
    "                        if num_fp >= limit:\n",
    "                            shouldStop = True\n",
    "                            break\n",
    "                        if num_im >= per_image_limit:\n",
    "                            nextImage = True\n",
    "                            break\n",
    "                \n",
    "                if shouldStop or nextImage:\n",
    "                    break\n",
    "\n",
    "            if shouldStop or nextImage:\n",
    "                break\n",
    "\n",
    "            new_height = math.ceil(transformed_image.size()[1] * rescale)\n",
    "            new_width = math.ceil(transformed_image.size()[2] * rescale)\n",
    "\n",
    "            # stop the loop if the image is smaller than the retina\n",
    "            if new_height < 36 or new_width < 36:\n",
    "                break\n",
    "\n",
    "            transformed_image = T.Resize((new_height, new_width), interpolation=InterpolationMode.BILINEAR)(transformed_image)\n",
    "        \n",
    "        if shouldStop:\n",
    "            break\n",
    "        \n",
    "        i += 1\n",
    "        i = (i % len(background_images))\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    images = images.reshape(images.size()[0], 1, 36, 36).permute((0, 2, 3, 1)).numpy()\n",
    "    return images\n",
    "\n",
    "TEXTURE_FP_FOLDER = os.path.abspath('./deep_learning_project/texturesfp/')\n",
    "def save_images(dir_path, images):\n",
    "    dir_path = os.path.join(dir_path, '0')\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    offset = len(os.listdir(dir_path))\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        filename = str(offset + i) + \".pgm\"\n",
    "        cv.imwrite(os.path.join(dir_path, filename), img)\n",
    "\n",
    "def delete_files_in_dir(dir_path):\n",
    "    onlyfiles = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "    for file in onlyfiles:\n",
    "        os.remove(os.path.join(dir_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epochs=10\n",
    "epochs=20\n",
    "learning_rate=0.0005\n",
    "momentum=0.90\n",
    "weight_decay=0\n",
    "valid_size=0.2\n",
    "batch_size=64\n",
    "\n",
    "bootstrap_min_epoch=3\n",
    "bootstrap_max_epoch=10\n",
    "bootstrap_lr = learning_rate * 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data is separated in 3 datasets.\n",
    "\n",
    "Train : to train the ML model.\n",
    "\n",
    "Valid : to valid the ML model.\n",
    "\n",
    "Test : to test the ML model.\n",
    "\n",
    "What is the difference between valid and test datasets. The main differencec is when there are used : valid are used inside the training process but test are used when the training is complete. Why use different datasets to do the same thing (test the generalization of model) ? Some do the validation with the test dataset but it is not scientifically correct because it will include a bias on the model. If we train the model until the test dataset error is the lowest, we effectively train the model for the test dataset... This is why we use two different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0. Parallel=False.\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "parallel = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        parallel = True\n",
    "\n",
    "print(f\"Running on {device}. Parallel={parallel}.\")\n",
    "\n",
    "data = imbalanced_load(valid_size=valid_size, batch_size=batch_size, device=device)\n",
    "train_loader = data[0]\n",
    "valid_loader = data[1]\n",
    "test_loader = data[2]\n",
    "classes = data[3]\n",
    "train_data = data[4]\n",
    "train_idx = data[5]\n",
    "\n",
    "background_images = load_background_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing one image of the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the FirstNeuralNetwork network\n",
    "\n",
    "We initialize the network.\n",
    "\n",
    "Print the configuration.\n",
    "\n",
    "Then predict a random input.\n",
    "\n",
    "---\n",
    "\n",
    "FirstNeuralNetwork is the neural network given by the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FourthNeuralNetwork(\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (fc2): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = FourthNeuralNetwork()\n",
    "if parallel:\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter = Exporter()\n",
    "exporter.prepare_export(MODEL_FOLDERS, str(model.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "trainer = BaseTrainer(model, loss_fn, optimizer, checkpoints_path=exporter.folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset=73376, train batches=1147, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [01:25, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 of 20 : train_loss: 0.55311, train_accuracy: 71.89%, valid_loss: 0.40419, valid_accuracy: 84.29%, test_loss: 0.23297, test_accuracy: 90.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:44, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 of 20 : train_loss: 0.40680, train_accuracy: 81.73%, valid_loss: 0.36516, valid_accuracy: 85.31%, test_loss: 0.18222, test_accuracy: 91.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:28, 40.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 20 : train_loss: 0.30076, train_accuracy: 87.44%, valid_loss: 0.22037, valid_accuracy: 91.24%, test_loss: 0.18023, test_accuracy: 92.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:23, 49.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 of 20 : train_loss: 0.23240, train_accuracy: 91.05%, valid_loss: 0.20990, valid_accuracy: 92.65%, test_loss: 0.09823, test_accuracy: 96.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:20, 55.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 of 20 : train_loss: 0.19386, train_accuracy: 92.75%, valid_loss: 0.22261, valid_accuracy: 91.80%, test_loss: 0.08210, test_accuracy: 97.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:19, 60.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 of 20 : train_loss: 0.16887, train_accuracy: 93.73%, valid_loss: 0.16401, valid_accuracy: 94.27%, test_loss: 0.07448, test_accuracy: 97.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:18, 61.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 of 20 : train_loss: 0.14568, train_accuracy: 94.68%, valid_loss: 0.11952, valid_accuracy: 95.76%, test_loss: 0.08746, test_accuracy: 97.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:18, 63.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 of 20 : train_loss: 0.13394, train_accuracy: 95.10%, valid_loss: 0.11327, valid_accuracy: 96.12%, test_loss: 0.06636, test_accuracy: 97.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:17, 64.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 of 20 : train_loss: 0.12088, train_accuracy: 95.60%, valid_loss: 0.09580, valid_accuracy: 96.57%, test_loss: 0.06548, test_accuracy: 97.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:18, 62.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 of 20 : train_loss: 0.11235, train_accuracy: 95.96%, valid_loss: 0.09366, valid_accuracy: 96.61%, test_loss: 0.05984, test_accuracy: 98.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:18, 62.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 of 20 : train_loss: 0.10853, train_accuracy: 96.10%, valid_loss: 0.08459, valid_accuracy: 97.03%, test_loss: 0.06853, test_accuracy: 97.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:18, 63.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 of 20 : train_loss: 0.10114, train_accuracy: 96.33%, valid_loss: 0.08093, valid_accuracy: 97.20%, test_loss: 0.07250, test_accuracy: 97.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:18, 62.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 of 20 : train_loss: 0.09655, train_accuracy: 96.55%, valid_loss: 0.09504, valid_accuracy: 96.68%, test_loss: 0.06346, test_accuracy: 97.80%\n",
      "CPU times: total: 1min 50s\n",
      "Wall time: 12min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            test_loader=test_loader,\n",
    "            min_epochs=min_epochs,\n",
    "            max_epochs=epochs,\n",
    "            early_stopping='valid',\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_equal_or_less(dataset, num_samples):\n",
    "    # find indices or face in training set and shuffle the indices\n",
    "    indices = [idx for idx, target in enumerate(dataset.targets) if target == 1]\n",
    "    indices = [idx for idx in indices if idx in train_idx]\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # use the same number if fp_data is small or use the whole set\n",
    "    if num_samples < len(indices):\n",
    "        indices = indices[:num_samples]\n",
    "    \n",
    "    return torch.utils.data.Subset(dataset, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting bootstrapping threshold=0.9\n",
      "Size of train dataset=121376, train batches=1897, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1897it [00:32, 58.13it/s]\n",
      "1897it [00:29, 63.53it/s]\n",
      "1897it [00:30, 62.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 10 : train_loss: 0.04646, train_accuracy: 98.41%, valid_loss: 0.07573, valid_accuracy: 97.14%, test_loss: 0.07755, test_accuracy: 97.51%\n",
      "\n",
      "Starting bootstrapping threshold=0.8\n",
      "Size of train dataset=125376, train batches=1959, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1959it [00:31, 62.22it/s]\n",
      "1959it [00:31, 62.33it/s]\n",
      "1959it [00:30, 63.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 10 : train_loss: 0.04476, train_accuracy: 98.46%, valid_loss: 0.07464, valid_accuracy: 97.38%, test_loss: 0.07548, test_accuracy: 97.55%\n",
      "\n",
      "Starting bootstrapping threshold=0.7\n",
      "Size of train dataset=129376, train batches=2022, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022it [00:56, 35.62it/s]\n",
      "2022it [00:41, 48.46it/s]\n",
      "2022it [00:35, 57.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 10 : train_loss: 0.04300, train_accuracy: 98.51%, valid_loss: 0.07612, valid_accuracy: 97.14%, test_loss: 0.07581, test_accuracy: 97.56%\n",
      "\n",
      "Starting bootstrapping threshold=0.6\n",
      "Size of train dataset=133376, train batches=2084, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2084it [00:34, 60.75it/s]\n",
      "2084it [00:32, 63.74it/s]\n",
      "2084it [00:32, 63.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 10 : train_loss: 0.04080, train_accuracy: 98.57%, valid_loss: 0.07285, valid_accuracy: 97.30%, test_loss: 0.07601, test_accuracy: 97.55%\n",
      "\n",
      "Starting bootstrapping threshold=0.5\n",
      "Size of train dataset=137376, train batches=2147, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2147it [00:34, 62.89it/s]\n",
      "2147it [00:33, 63.74it/s]\n",
      "2147it [00:33, 63.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 10 : train_loss: 0.04073, train_accuracy: 98.59%, valid_loss: 0.07489, valid_accuracy: 97.27%, test_loss: 0.07672, test_accuracy: 97.54%\n",
      "\n",
      "Starting bootstrapping threshold=0.4\n",
      "Size of train dataset=141376, train batches=2209, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2209it [01:48, 20.30it/s]\n",
      "2209it [01:01, 36.00it/s]\n",
      "2209it [00:44, 49.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 10 : train_loss: 0.03944, train_accuracy: 98.63%, valid_loss: 0.07638, valid_accuracy: 97.24%, test_loss: 0.07858, test_accuracy: 97.48%\n",
      "\n",
      "Starting bootstrapping threshold=0.3\n",
      "Size of train dataset=145376, train batches=2272, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2272it [01:30, 25.11it/s]\n",
      "2272it [01:14, 30.36it/s]\n",
      "2272it [00:50, 44.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 10 : train_loss: 0.03853, train_accuracy: 98.66%, valid_loss: 0.07759, valid_accuracy: 97.03%, test_loss: 0.08245, test_accuracy: 97.27%\n",
      "\n",
      "Starting bootstrapping threshold=0.2\n",
      "Size of train dataset=149376, train batches=2334, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2334it [00:36, 64.12it/s]\n",
      "2334it [00:36, 63.35it/s]\n",
      "2334it [00:36, 63.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 10 : train_loss: 0.03776, train_accuracy: 98.70%, valid_loss: 0.07696, valid_accuracy: 97.12%, test_loss: 0.07872, test_accuracy: 97.46%\n",
      "\n",
      "Starting bootstrapping threshold=0.1\n",
      "Size of train dataset=153376, train batches=2397, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2397it [00:37, 64.13it/s]\n",
      "2397it [00:37, 63.97it/s]\n",
      "2397it [00:37, 63.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 10 : train_loss: 0.03746, train_accuracy: 98.70%, valid_loss: 0.07921, valid_accuracy: 97.08%, test_loss: 0.08421, test_accuracy: 97.10%\n",
      "\n",
      "Starting bootstrapping threshold=0\n",
      "Size of train dataset=157376, train batches=2459, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2459it [00:38, 63.55it/s]\n",
      "2459it [00:38, 64.06it/s]\n",
      "2459it [00:38, 63.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 10 : train_loss: 0.03643, train_accuracy: 98.72%, valid_loss: 0.07729, valid_accuracy: 97.08%, test_loss: 0.08571, test_accuracy: 97.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.path.join(TEXTURE_FP_FOLDER, \"0\")\n",
    "# delete_files_in_dir(dir_path)\n",
    "#  \n",
    "for threshold in [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0]:\n",
    "    print(\"Starting bootstrapping threshold={0}\".format(str(threshold)))\n",
    "    fp_images = gather_false_positive(\n",
    "        model,\n",
    "        background_images,\n",
    "        rescale=0.8,\n",
    "        stride=36,\n",
    "        threshold=threshold,\n",
    "        limit=4000,\n",
    "        per_image_limit=4000/len(background_images),\n",
    "        shuffle=True,\n",
    "        device=device)\n",
    "    \n",
    "    save_images(TEXTURE_FP_FOLDER, fp_images)\n",
    "\n",
    "    fp_data = torchvision.datasets.ImageFolder(TEXTURE_FP_FOLDER, transform=get_both_transform())\n",
    "    \n",
    "    # face_imgs = make_dataset_equal_or_less(train_data, len(fp_data))\n",
    "    face_imgs = torch.utils.data.Subset(train_data, train_idx)\n",
    "\n",
    "    both_data = torch.utils.data.ConcatDataset([face_imgs, fp_data])\n",
    "    both_sampler = ImbalancedDatasetSampler(both_data, callback_get_label=(lambda x, y: x[y][1]))\n",
    "\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if 'cuda' in device else {}\n",
    "    loader = torch.utils.data.DataLoader(both_data, batch_size=batch_size, sampler=both_sampler, **kwargs)\n",
    "    \n",
    "    # trainer.optimizer = torch.optim.Adam(model.parameters(), lr=bootstrap_lr, weight_decay=weight_decay)\n",
    "    trainer.optimizer = torch.optim.SGD(model.parameters(), lr=bootstrap_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "    trainer.fit(train_loader=loader,\n",
    "            valid_loader=valid_loader,\n",
    "            test_loader=test_loader,\n",
    "            min_epochs=bootstrap_min_epoch,\n",
    "            max_epochs=bootstrap_max_epoch,\n",
    "            early_stopping='valid',\n",
    "            device=device,\n",
    "            verbose=False)\n",
    "\n",
    "    trainer._print_evaluation(valid_loader, test_loader)\n",
    "    print()\n",
    "\n",
    "# fullset = torchvision.datasets.ImageFolder(TEXTURE_FP_FOLDER, transform=get_transform())\n",
    "# both_data_full = torch.utils.data.ConcatDataset([torch.utils.data.Subset(train_data, train_idx), fp_data])\n",
    "# full_both_sampler = ImbalancedDatasetSampler(both_data_full, callback_get_label=(lambda x, y: x[y][1]))\n",
    "# kwargs = {'num_workers': 4, 'pin_memory': True} if 'cuda' in device else {}\n",
    "# full_loader = torch.utils.data.DataLoader(both_data_full, batch_size=batch_size, sampler=full_both_sampler, **kwargs)\n",
    "\n",
    "# trainer.fit(train_loader=full_loader,\n",
    "#             valid_loader=valid_loader,\n",
    "#             test_loader=test_loader,\n",
    "#             epochs=epochs,\n",
    "#             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file_data = {\n",
    "    \"device\": device,\n",
    "    \"network\": str(model.__class__.__name__),\n",
    "    \"epochs_number\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"train_len_data\": len(train_loader.dataset), \n",
    "    \"test_len_data\": len(test_loader.dataset),\n",
    "    \"trainer\": str(trainer.__class__.__name__),\n",
    "    \"optimizer\": str(optimizer.__class__.__name__),\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"loss_function\": str(loss_fn.__class__.__name__),\n",
    "    \"performances\" : trainer.get_stats(),\n",
    "}\n",
    "\n",
    "exporter.export_stat_file(stats_file_data)\n",
    "\n",
    "exporter.export_model(model, 'weights.pt')\n",
    "exporter.export_best_models(trainer.get_best_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# ld_model = torch.load('model.pt')\n",
    "\n",
    "# test_loop(test_loader, ld_model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c19fa61d258bb2b35aae2ada233c33e2817c1ce895aa48acba720c6bf7cbe3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
