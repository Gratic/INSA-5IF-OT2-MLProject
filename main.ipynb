{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of the project is to recognize if the image is a face or not.\n",
    "\n",
    "Images are greyscale 36x36 pixels images.\n",
    "\n",
    "To reach the goal, we will try to train a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_learning_project.load_data import basic_load\n",
    "from deep_learning_project.net import FirstNeuralNetwork, LinearRegressionNetwork\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from deep_learning_project.trainers import BaseTrainer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "CURRENT_FOLDER = '.'\n",
    "MODEL_FOLDERS = os.path.join(CURRENT_FOLDER, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data is separated in 3 datasets.\n",
    "\n",
    "Train : to train the ML model.\n",
    "\n",
    "Valid : to valid the ML model.\n",
    "\n",
    "Test : to test the ML model.\n",
    "\n",
    "What is the difference between valid and test datasets. The main differencec is when there are used : valid are used inside the training process but test are used when the training is complete. Why use different datasets to do the same thing (test the generalization of model) ? Some do the validation with the test dataset but it is not scientifically correct because it will include a bias on the model. If we train the model until the test dataset error is the lowest, we effectively train the model for the test dataset... This is why we use two different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "valid_size = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "print(f\"Running on {device}.\")\n",
    "\n",
    "data = basic_load(valid_size=valid_size, batch_size=batch_size, device=device)\n",
    "train_loader = data[0]\n",
    "valid_loader = data[1]\n",
    "test_loader = data[2]\n",
    "classes = data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing one image of the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(data[0]))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze() # from 2d to 1d, works only when the data is 1d [[x]] => [x]\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the FirstNeuralNetwork network\n",
    "\n",
    "We initialize the network.\n",
    "\n",
    "Print the configuration.\n",
    "\n",
    "Then predict a random input.\n",
    "\n",
    "---\n",
    "\n",
    "FirstNeuralNetwork is the neural network given by the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = LinearRegressionNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_shit = torch.rand((1, 1, 36, 36), device=device)\n",
    "\n",
    "logits = model(random_shit)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_now = datetime.datetime.now()\n",
    "model_folder_name = os.path.join(MODEL_FOLDERS, date_now.strftime(\"%Y%m%d_%H%M\") + '_' + str(model.__class__.__name__))\n",
    "os.makedirs(model_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "trainer = BaseTrainer(model, loss_fn, optimizer, checkpoints_path=model_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            epochs=epochs,\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(best_epoch, best_accuracy, best_loss, best_model) = trainer.get_best_model()\n",
    "\n",
    "stats_file_data = {\n",
    "    \"device\": device,\n",
    "    \"network\": str(model.__class__.__name__),\n",
    "    \"epochs_number\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"train_len_data\": len(train_loader.dataset), \n",
    "    \"test_len_data\": len(test_loader.dataset),\n",
    "    \"trainer\": str(trainer.__class__.__name__),\n",
    "    \"optimizer\": str(optimizer.__class__.__name__),\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"loss_function\": str(loss_fn.__class__.__name__),\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_accuracy\": best_accuracy,\n",
    "    \"best_loss\": best_loss,\n",
    "    \"performances\" : trainer.get_stats(),\n",
    "}\n",
    "\n",
    "stats_file_data_json = json.dumps(stats_file_data, indent=4)\n",
    "\n",
    "stat_file_name = os.path.join(model_folder_name, 'stats.json')\n",
    "model_file_name = os.path.join(model_folder_name, 'weights.pt')\n",
    "best_model_file_name = os.path.join(model_folder_name, 'best_weights.pt')\n",
    "\n",
    "with open(os.path.normpath(stat_file_name), 'w', encoding = 'utf-8') as file:\n",
    "    file.write(stats_file_data_json)\n",
    "\n",
    "torch.save(model.state_dict(), model_file_name)\n",
    "torch.save(best_model, best_model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# ld_model = torch.load('model.pt')\n",
    "\n",
    "# test_loop(test_loader, ld_model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c19fa61d258bb2b35aae2ada233c33e2817c1ce895aa48acba720c6bf7cbe3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
