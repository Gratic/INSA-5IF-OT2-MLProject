{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of the project is to recognize if the image is a face or not.\n",
    "\n",
    "Images are greyscale 36x36 pixels images.\n",
    "\n",
    "To reach the goal, we will try to train a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deep_learning_project.load_data import basic_load, imbalanced_load, get_transform, get_both_transform\n",
    "from deep_learning_project.net import FirstNeuralNetwork, LinearRegressionNetwork, SecondNeuralNetwork, ThirdNeuralNetwork\n",
    "from deep_learning_project.torchsampler import ImbalancedDatasetSampler\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from deep_learning_project.trainers import BaseTrainer\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from deep_learning_project.utils import Exporter\n",
    "import cv2 as cv\n",
    "from cv2 import IMREAD_GRAYSCALE, IMREAD_COLOR\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "CURRENT_FOLDER = '.'\n",
    "MODEL_FOLDERS = os.path.join(CURRENT_FOLDER, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    obj = None\n",
    "    with torch.no_grad():\n",
    "        # exploit the model\n",
    "        logits = model(img)\n",
    "        pred_probab = nn.Softmax(dim=1)(logits)\n",
    "        y_pred = pred_probab.argmax(1).item() # indice(s) of the maximum value in the tensor\n",
    "        obj = (y_pred, pred_probab)\n",
    "    return obj\n",
    "\n",
    "def load_background_images():\n",
    "    TEXTURE_FOLDER = os.path.abspath('./deep_learning_project/textures/')\n",
    "\n",
    "    onlyfiles = [f for f in os.listdir(TEXTURE_FOLDER) if os.path.isfile(os.path.join(TEXTURE_FOLDER, f))]\n",
    "\n",
    "    img_datas = []\n",
    "\n",
    "    for filename in onlyfiles:\n",
    "        img_src = cv.samples.findFile(os.path.join(TEXTURE_FOLDER, filename))\n",
    "        img_datas.append(cv.imread(img_src, IMREAD_COLOR))\n",
    "    \n",
    "    return img_datas\n",
    "\n",
    "def gather_false_positive(model, background_images, rescale=0.8, threshold=0.8, stride=1, limit=1000, shuffle=True, device='cpu'):\n",
    "    images = [] # this array will contain false positives images\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(background_images)\n",
    "\n",
    "    for image in background_images:\n",
    "        transform = get_transform()\n",
    "        transformed_image = transform(T.ToPILImage()(image))\n",
    "\n",
    "        shouldStop = False\n",
    "        num_fp = 0\n",
    "\n",
    "        while (True):\n",
    "            for y in range(0, transformed_image.size()[1] - 36, stride):\n",
    "                for x in range(0, transformed_image.size()[2] - 36, stride):\n",
    "\n",
    "                    # crop and preparing the cropped image\n",
    "                    new_img = transformed_image[:, y:y+36, x:x+36]\n",
    "                    torch_new_img = new_img.reshape((1, 1, 36, 36))\n",
    "\n",
    "                    (y_pred, pred_probab) = predict(model, torch_new_img.to(device))\n",
    "                    \n",
    "                    # 0 = noface, 1 = face\n",
    "                    if pred_probab.squeeze()[1] >= threshold:\n",
    "                        images.append(new_img.reshape(36, 36))\n",
    "                        num_fp += 1\n",
    "                        if num_fp >= limit:\n",
    "                            shouldStop = True\n",
    "                            break\n",
    "                \n",
    "                if shouldStop:\n",
    "                    break\n",
    "\n",
    "            if shouldStop:\n",
    "                break\n",
    "                        \n",
    "\n",
    "            new_height = math.ceil(transformed_image.size()[1] * rescale)\n",
    "            new_width = math.ceil(transformed_image.size()[2] * rescale)\n",
    "\n",
    "            # stop the loop if the image is smaller than the retina\n",
    "            if new_height < 36 or new_width < 36:\n",
    "                break\n",
    "\n",
    "            transformed_image = T.Resize((new_height, new_width), interpolation=InterpolationMode.BILINEAR)(transformed_image)\n",
    "        \n",
    "        if shouldStop:\n",
    "            break\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    images = images.reshape(images.size()[0], 1, 36, 36).permute((0, 2, 3, 1)).numpy()\n",
    "    return images\n",
    "\n",
    "TEXTURE_FP_FOLDER = os.path.abspath('./deep_learning_project/texturesfp/')\n",
    "def save_images(dir_path, images):\n",
    "    dir_path = os.path.join(dir_path, '0')\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    offset = len(os.listdir(dir_path))\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        filename = str(offset + i) + \".pgm\"\n",
    "        cv.imwrite(os.path.join(dir_path, filename), img)\n",
    "\n",
    "def delete_files_in_dir(dir_path):\n",
    "    onlyfiles = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "    for file in onlyfiles:\n",
    "        os.remove(os.path.join(dir_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epochs=10\n",
    "epochs=20\n",
    "learning_rate=0.001\n",
    "momentum=0.90\n",
    "weight_decay=0\n",
    "valid_size=0.2\n",
    "batch_size=64\n",
    "\n",
    "bootstart_min_epoch=5\n",
    "bootstart_max_epoch=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data is separated in 3 datasets.\n",
    "\n",
    "Train : to train the ML model.\n",
    "\n",
    "Valid : to valid the ML model.\n",
    "\n",
    "Test : to test the ML model.\n",
    "\n",
    "What is the difference between valid and test datasets. The main differencec is when there are used : valid are used inside the training process but test are used when the training is complete. Why use different datasets to do the same thing (test the generalization of model) ? Some do the validation with the test dataset but it is not scientifically correct because it will include a bias on the model. If we train the model until the test dataset error is the lowest, we effectively train the model for the test dataset... This is why we use two different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0. Parallel=False.\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "parallel = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        parallel = True\n",
    "\n",
    "print(f\"Running on {device}. Parallel={parallel}.\")\n",
    "\n",
    "data = imbalanced_load(valid_size=valid_size, batch_size=batch_size, device=device)\n",
    "train_loader = data[0]\n",
    "valid_loader = data[1]\n",
    "test_loader = data[2]\n",
    "classes = data[3]\n",
    "train_data = data[4]\n",
    "train_idx = data[5]\n",
    "\n",
    "background_images = load_background_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 36, 36])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(data[0]))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze() # from 2d to 1d, works only when the data is 1d [[x]] => [x]\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing one image of the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the FirstNeuralNetwork network\n",
    "\n",
    "We initialize the network.\n",
    "\n",
    "Print the configuration.\n",
    "\n",
    "Then predict a random input.\n",
    "\n",
    "---\n",
    "\n",
    "FirstNeuralNetwork is the neural network given by the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThirdNeuralNetwork(\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(1, 72, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(72, 144, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Conv2d(144, 256, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (fc2): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = ThirdNeuralNetwork()\n",
    "if parallel:\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([[[0]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "random_shit = torch.rand((1, 1, 36, 36), device=device)\n",
    "\n",
    "logits = model(random_shit)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptd = logits.reshape((1, 2))\n",
    "targer = torch.Tensor([1, 0]).reshape((1,2))\n",
    "loss = loss_fn(ptd.cpu(), targer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter = Exporter()\n",
    "exporter.prepare_export(MODEL_FOLDERS, str(model.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "trainer = BaseTrainer(model, loss_fn, optimizer, checkpoints_path=exporter.folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset=73376, train batches=1147, valid dataset=18344, valid batches=287, test dataset=7628, test batches=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [01:03, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 of 20 : train_loss: 0.17715, train_accuracy: 92.67%, valid_loss: 0.10329, valid_accuracy: 96.30%, test_loss: 0.07196, test_accuracy: 97.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:36, 31.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 of 20 : train_loss: 0.07151, train_accuracy: 97.37%, valid_loss: 0.06506, valid_accuracy: 97.73%, test_loss: 0.06340, test_accuracy: 97.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:29, 39.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 of 20 : train_loss: 0.04828, train_accuracy: 98.28%, valid_loss: 0.05087, valid_accuracy: 98.24%, test_loss: 0.07671, test_accuracy: 97.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:35, 32.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 of 20 : train_loss: 0.03789, train_accuracy: 98.67%, valid_loss: 0.03658, valid_accuracy: 98.76%, test_loss: 0.09223, test_accuracy: 97.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:46, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 of 20 : train_loss: 0.03233, train_accuracy: 98.87%, valid_loss: 0.02925, valid_accuracy: 98.92%, test_loss: 0.10832, test_accuracy: 96.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:34, 33.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 of 20 : train_loss: 0.02670, train_accuracy: 99.04%, valid_loss: 0.02618, valid_accuracy: 99.09%, test_loss: 0.12093, test_accuracy: 96.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:31, 36.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 of 20 : train_loss: 0.02286, train_accuracy: 99.21%, valid_loss: 0.02119, valid_accuracy: 99.21%, test_loss: 0.09583, test_accuracy: 96.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:32, 35.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 of 20 : train_loss: 0.02229, train_accuracy: 99.24%, valid_loss: 0.02241, valid_accuracy: 99.21%, test_loss: 0.12094, test_accuracy: 95.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:34, 33.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 of 20 : train_loss: 0.01921, train_accuracy: 99.36%, valid_loss: 0.01452, valid_accuracy: 99.48%, test_loss: 0.09097, test_accuracy: 97.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [00:32, 34.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 of 20 : train_loss: 0.01861, train_accuracy: 99.37%, valid_loss: 0.01729, valid_accuracy: 99.37%, test_loss: 0.13021, test_accuracy: 95.63%\n",
      "CPU times: total: 3min 12s\n",
      "Wall time: 12min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            test_loader=test_loader,\n",
    "            min_epochs=min_epochs,\n",
    "            max_epochs=epochs,\n",
    "            early_stopping='valid',\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_equal_or_less(dataset, num_samples):\n",
    "    # find indices or face in training set and shuffle the indices\n",
    "    indices = [idx for idx, target in enumerate(dataset.targets) if target == 1]\n",
    "    indices = [idx for idx in indices if idx in train_idx]\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # use the same number if fp_data is small or use the whole set\n",
    "    if num_samples < len(indices):\n",
    "        indices = indices[:num_samples]\n",
    "    \n",
    "    return torch.utils.data.Subset(dataset, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_path = os.path.join(TEXTURE_FP_FOLDER, \"0\")\n",
    "# delete_files_in_dir(dir_path)\n",
    "\n",
    "# for threshold in [0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0]:\n",
    "#     print(\"Starting bootstrapping threshold={0}\".format(str(threshold)))\n",
    "#     fp_images = gather_false_positive(\n",
    "#         model,\n",
    "#         background_images,\n",
    "#         rescale=0.8,\n",
    "#         stride=1,\n",
    "#         threshold=threshold,\n",
    "#         limit=1000,\n",
    "#         shuffle=True,\n",
    "#         device=device)\n",
    "    \n",
    "#     save_images(TEXTURE_FP_FOLDER, fp_images)\n",
    "\n",
    "#     fp_data = torchvision.datasets.ImageFolder(TEXTURE_FP_FOLDER, transform=get_both_transform())\n",
    "    \n",
    "#     # face_imgs = make_dataset_equal_or_less(train_data, len(fp_data))\n",
    "#     face_imgs = torch.utils.data.Subset(train_data, train_idx)\n",
    "\n",
    "#     both_data = torch.utils.data.ConcatDataset([face_imgs, fp_data])\n",
    "#     both_sampler = ImbalancedDatasetSampler(both_data, callback_get_label=(lambda x, y: x[y][1]))\n",
    "\n",
    "#     kwargs = {'num_workers': 4, 'pin_memory': True} if 'cuda' in device else {}\n",
    "#     loader = torch.utils.data.DataLoader(both_data, batch_size=batch_size, sampler=both_sampler, **kwargs)\n",
    "    \n",
    "#     trainer.fit(train_loader=loader,\n",
    "#             valid_loader=valid_loader,\n",
    "#             test_loader=test_loader,\n",
    "#             min_epochs=bootstart_min_epoch,\n",
    "#             max_epochs=bootstart_max_epoch,\n",
    "#             early_stopping='valid',\n",
    "#             device=device,\n",
    "#             verbose=False)\n",
    "\n",
    "#     trainer._print_evaluation(valid_loader, test_loader)\n",
    "#     print()\n",
    "\n",
    "# # fullset = torchvision.datasets.ImageFolder(TEXTURE_FP_FOLDER, transform=get_transform())\n",
    "# # both_data_full = torch.utils.data.ConcatDataset([torch.utils.data.Subset(train_data, train_idx), fp_data])\n",
    "# # full_both_sampler = ImbalancedDatasetSampler(both_data_full, callback_get_label=(lambda x, y: x[y][1]))\n",
    "# # kwargs = {'num_workers': 4, 'pin_memory': True} if 'cuda' in device else {}\n",
    "# # full_loader = torch.utils.data.DataLoader(both_data_full, batch_size=batch_size, sampler=full_both_sampler, **kwargs)\n",
    "\n",
    "# # trainer.fit(train_loader=full_loader,\n",
    "# #             valid_loader=valid_loader,\n",
    "# #             test_loader=test_loader,\n",
    "# #             epochs=epochs,\n",
    "# #             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file_data = {\n",
    "    \"device\": device,\n",
    "    \"network\": str(model.__class__.__name__),\n",
    "    \"epochs_number\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"train_len_data\": len(train_loader.dataset), \n",
    "    \"test_len_data\": len(test_loader.dataset),\n",
    "    \"trainer\": str(trainer.__class__.__name__),\n",
    "    \"optimizer\": str(optimizer.__class__.__name__),\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"loss_function\": str(loss_fn.__class__.__name__),\n",
    "    \"performances\" : trainer.get_stats(),\n",
    "}\n",
    "\n",
    "exporter.export_stat_file(stats_file_data)\n",
    "\n",
    "exporter.export_model(model, 'weights.pt')\n",
    "exporter.export_best_models(trainer.get_best_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# ld_model = torch.load('model.pt')\n",
    "\n",
    "# test_loop(test_loader, ld_model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c19fa61d258bb2b35aae2ada233c33e2817c1ce895aa48acba720c6bf7cbe3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
