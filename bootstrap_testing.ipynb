{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from cv2 import IMREAD_GRAYSCALE, IMREAD_COLOR\n",
    "from deep_learning_project.load_data import get_transform\n",
    "from deep_learning_project.net import LinearRegressionNetwork, SecondNeuralNetwork\n",
    "from torch import nn\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SecondNeuralNetwork()\n",
    "\n",
    "model.load_state_dict(torch.load('models/20221026_1659_SecondNeuralNetwork/best_test_weights.pt'))\n",
    "model.eval()\n",
    "\n",
    "classes = ['noface', 'face']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    obj = None\n",
    "    with torch.no_grad():\n",
    "        # exploit the model\n",
    "        logits = model(img)\n",
    "        pred_probab = nn.Softmax(dim=1)(logits)\n",
    "        y_pred = pred_probab.argmax(1).item() # indice(s) of the maximum value in the tensor\n",
    "        obj = (y_pred, pred_probab)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "def load_background_images():\n",
    "    TEXTURE_FOLDER = os.path.abspath('./deep_learning_project/textures/')\n",
    "\n",
    "    onlyfiles = [f for f in os.listdir(TEXTURE_FOLDER) if os.path.isfile(os.path.join(TEXTURE_FOLDER, f))]\n",
    "\n",
    "    img_datas = []\n",
    "\n",
    "    for filename in onlyfiles:\n",
    "        img_src = cv.samples.findFile(os.path.join(TEXTURE_FOLDER, filename))\n",
    "        img_datas.append(cv.imread(img_src, IMREAD_COLOR))\n",
    "    \n",
    "    return img_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_false_positive(background_images, rescale=0.8, threshold=0.8, stride=1):\n",
    "    images = [] # this array will contain false positives images\n",
    "\n",
    "    for image in background_images:\n",
    "        transform = get_transform()\n",
    "        transformed_image = transform(T.ToPILImage()(image))\n",
    "\n",
    "        while (True):\n",
    "            for y in range(0, transformed_image.size()[1] - 36, stride):\n",
    "                for x in range(0, transformed_image.size()[2] - 36, stride):\n",
    "\n",
    "                    # crop and preparing the cropped image\n",
    "                    new_img = transformed_image[:, y:y+36, x:x+36]\n",
    "                    torch_new_img = new_img.reshape((1, 1, 36, 36))\n",
    "\n",
    "                    (y_pred, pred_probab) = predict(model, torch_new_img)\n",
    "                    \n",
    "                    # 0 = noface, 1 = face\n",
    "                    if(y_pred == 1 and (threshold == None or pred_probab.squeeze()[1] >= threshold)):\n",
    "                        images.append(new_img.reshape(36, 36))\n",
    "\n",
    "            new_height = math.ceil(transformed_image.size()[1] * rescale)\n",
    "            new_width = math.ceil(transformed_image.size()[2] * rescale)\n",
    "\n",
    "            # stop the loop if the image is smaller than the retina\n",
    "            if new_height < 36 or new_width < 36:\n",
    "                break\n",
    "\n",
    "            transformed_image = T.Resize((new_height, new_width), interpolation=InterpolationMode.BILINEAR)(transformed_image)\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    images = images.reshape(40, 1, 36, 36).permute((0, 2, 3, 1)).numpy()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTURE_FP_FOLDER = os.path.abspath('./deep_learning_project/texturesfp/0/')\n",
    "def save_images(dir_path, images):\n",
    "\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        filename = str(i) + \".pgm\"\n",
    "        cv.imwrite(os.path.join(dir_path, filename), img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c19fa61d258bb2b35aae2ada233c33e2817c1ce895aa48acba720c6bf7cbe3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
